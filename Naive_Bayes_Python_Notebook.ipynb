{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naive Bayes Python Notebook",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMmQKtPeU3ruBvtg8cgtJJI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kandeo/Core-Week-9/blob/main/Naive_Bayes_Python_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMAnFYdbqeS8"
      },
      "source": [
        "# **Spam Classification Predictive Model**\n",
        "\n",
        "**Overview** \n",
        "\n",
        "This project involves implementing a Naive Bayes classifier to classify emails as spam and non-spam. Once the experiment is conducted, the resulting metrics are to be calculated.\n",
        "\n",
        "Experimental Procedure:\n",
        "\n",
        "1. Downloading the dataset.\n",
        "2. Randomly partitioning the dataset into two parts; 80-20 sets.\n",
        "5. Computing the accuracy (percentage of correct classification).\n",
        "6. Reporting the confusion matrix of the classifier.\n",
        "7. Repeating step 2 to step 4 twice, each time splitting the dataset differently i.e. 70-30, 60-40, then noting the outcomes of the modeling.\n",
        "8. Suggesting and applying at least one of the optimization techniques learned.\n",
        "9. Providing further recommendations to improve the classifier.\n",
        "\n",
        "The dataset has been taken from the UCI ML repository and contains about 4,600 emails labelled as spam or non-spam.\n",
        "\n",
        "Dataset: https://archive.ics.uci.edu/ml/datasets/Spambase "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnZ9PzoVuqpi"
      },
      "source": [
        "# **Defining the Question**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-TgEWuUuxpD"
      },
      "source": [
        "**Specific Data Analytics Question**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUhM_b-du4we"
      },
      "source": [
        "Building a Naive bayes model that determines whether a given email is spam or non-spam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5xt5fNIvbgO"
      },
      "source": [
        "**Metrics for success**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKrAV6O0qPY-"
      },
      "source": [
        "The model should have a true positive rate of 90% and above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7At8TvEuv92c"
      },
      "source": [
        "**Understanding the Context**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3-GOI45rmat"
      },
      "source": [
        "Communication by email is one of the most important means of communication in the cooperate and private setups. An internet user is constantly confronted by mass emails containing undesired information. Spam emails are generally defined as bulk unsolicited emailing, generally commercial in nature, and predominately fraudulent.\n",
        "\n",
        "The Spambase dataset collected at Hewlett-Packard Labs, classifies 4601 emails as spam or non-spam. In addition to this class label there are 57 variables indicating the frequency of certain words and characters in the email.\n",
        "\n",
        "The collection of spam emails came from the collectors postmaster and individuals who had filed spam. The collection of non-spam emails came from filed work and personal emails hence the word 'george' and the area code '650' are indicators of non-spam. These are useful when constructing a personalized spam filter. One would either have to blind such non-spam indicators or get a very wide collection of non-spam to generate a general purpose spam filter.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUiG-AOzxUUr"
      },
      "source": [
        "**Experimental Design**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtSmlkgzxXxf"
      },
      "source": [
        "1. Data Preparation\n",
        " - Data Loading\n",
        " - Data Uniformity\n",
        " - Handling missing/duplicate values\n",
        "3. Data Exploration\n",
        " - Univariate analysis\n",
        "4. Modelling\n",
        " - Instantiating the models\n",
        " - Performance Evaluation\n",
        " - Hyperparameter Tuning\n",
        " - Prediction\n",
        "5. Performance Analysis\n",
        " - Assessing Accuracy\n",
        " - Recommendations and Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Relevance**"
      ],
      "metadata": {
        "id": "UlqnRINtUYPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The first 48 variables contain the frequency of the variable name in the e-mail, E.g: business, our, make, over, all...\n",
        "\n",
        "- If the variable name starts with num, E.g: num650 it indicates the frequency of the corresponding number 650. \n",
        "\n",
        "- The variables 49-54 indicate the frequency of the characters. E.g ; , ( , [ , ! , $ , #. \n",
        "\n",
        "- The variables 55-57 contain the average, longest and total run-length of capital letters. \n",
        "\n",
        "- Variable 58 indicates the type of the mail either Spam - 1 or Non-Spam - 0.\n",
        "\n"
      ],
      "metadata": {
        "id": "voWW11LtUcv0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4yZGIHO0qRZ"
      },
      "source": [
        "# **Data Preparation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pe3DJ9XQJ4z7"
      },
      "source": [
        "Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1lLWPMJ55_t"
      },
      "source": [
        "#Importing the respective dependencies:\n",
        "\n",
        "#1. For Data wrangling and visualization:\n",
        "import pandas as pd \n",
        "import numpy as np      \n",
        "import seaborn as sns   \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#2. Machine Learning Models:\n",
        "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
        "\n",
        "#3. For Model Evaluation:\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
        "\n",
        "#4. Removing Warnings:\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25YfqoDtKbBD"
      },
      "source": [
        "Loading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Naming the respective columns:\n",
        "cols = [\"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\", \"word_freq_our\", \"word_freq_over\", \"word_freq_remove\",\n",
        "        \"word_freq_internet\", \"word_freq_order\", \"word_freq_mail\", \"word_freq_receive\", \"word_freq_will\", \"word_freq_people\", \n",
        "        \"word_freq_report\", \"word_freq_addresses\", \"word_freq_free\", \"word_freq_business\", \"word_freq_email\", \"word_freq_you\", \n",
        "        \"word_freq_credit\", \"word_freq_your\", \"word_freq_font\", \"word_freq_000\", \"word_freq_money\", \"word_freq_hp\", \"word_freq_hpl\", \n",
        "        \"word_freq_george\", \"word_freq_650\", \"word_freq_lab\", \"word_freq_labs\",  \"word_freq_telnet\", \"word_freq_857\", \"word_freq_data\", \n",
        "        \"word_freq_415\", \"word_freq_85\", \"word_freq_technology\", \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\", \"word_freq_direct\",\n",
        "        \"word_freq_cs\", \"word_freq_meeting\", \"word_freq_original\", \"word_freq_project\", \"word_freq_re\", \"word_freq_edu\", \"word_freq_table\",\n",
        "        \"word_freq_conference\", \"char_freq_;\", \"char_freq_(\", \"char_freq_[\", \"char_freq_!\", \"char_freq_$\", \"char_freq_#\", \n",
        "        \"capital_run_length_average\", \"capital_run_length_longest\", \"capital_run_length_total\", \"spam\"]\n",
        "\n",
        "#Loading the dataset from the UCI repository:\n",
        "Spam = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data', names=cols, header=None)\n",
        "\n",
        "#Printing the first 4 rows of the data:\n",
        "Spam.head(4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "aLqMhUQ_qaDU",
        "outputId": "4ed35843-f762-488b-e02e-a079219aa185"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_freq_make</th>\n",
              "      <th>word_freq_address</th>\n",
              "      <th>word_freq_all</th>\n",
              "      <th>word_freq_3d</th>\n",
              "      <th>word_freq_our</th>\n",
              "      <th>word_freq_over</th>\n",
              "      <th>word_freq_remove</th>\n",
              "      <th>word_freq_internet</th>\n",
              "      <th>word_freq_order</th>\n",
              "      <th>word_freq_mail</th>\n",
              "      <th>word_freq_receive</th>\n",
              "      <th>word_freq_will</th>\n",
              "      <th>word_freq_people</th>\n",
              "      <th>word_freq_report</th>\n",
              "      <th>word_freq_addresses</th>\n",
              "      <th>word_freq_free</th>\n",
              "      <th>word_freq_business</th>\n",
              "      <th>word_freq_email</th>\n",
              "      <th>word_freq_you</th>\n",
              "      <th>word_freq_credit</th>\n",
              "      <th>word_freq_your</th>\n",
              "      <th>word_freq_font</th>\n",
              "      <th>word_freq_000</th>\n",
              "      <th>word_freq_money</th>\n",
              "      <th>word_freq_hp</th>\n",
              "      <th>word_freq_hpl</th>\n",
              "      <th>word_freq_george</th>\n",
              "      <th>word_freq_650</th>\n",
              "      <th>word_freq_lab</th>\n",
              "      <th>word_freq_labs</th>\n",
              "      <th>word_freq_telnet</th>\n",
              "      <th>word_freq_857</th>\n",
              "      <th>word_freq_data</th>\n",
              "      <th>word_freq_415</th>\n",
              "      <th>word_freq_85</th>\n",
              "      <th>word_freq_technology</th>\n",
              "      <th>word_freq_1999</th>\n",
              "      <th>word_freq_parts</th>\n",
              "      <th>word_freq_pm</th>\n",
              "      <th>word_freq_direct</th>\n",
              "      <th>word_freq_cs</th>\n",
              "      <th>word_freq_meeting</th>\n",
              "      <th>word_freq_original</th>\n",
              "      <th>word_freq_project</th>\n",
              "      <th>word_freq_re</th>\n",
              "      <th>word_freq_edu</th>\n",
              "      <th>word_freq_table</th>\n",
              "      <th>word_freq_conference</th>\n",
              "      <th>char_freq_;</th>\n",
              "      <th>char_freq_(</th>\n",
              "      <th>char_freq_[</th>\n",
              "      <th>char_freq_!</th>\n",
              "      <th>char_freq_$</th>\n",
              "      <th>char_freq_#</th>\n",
              "      <th>capital_run_length_average</th>\n",
              "      <th>capital_run_length_longest</th>\n",
              "      <th>capital_run_length_total</th>\n",
              "      <th>spam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.29</td>\n",
              "      <td>1.93</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.778</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.756</td>\n",
              "      <td>61</td>\n",
              "      <td>278</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.21</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.28</td>\n",
              "      <td>3.47</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.59</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.372</td>\n",
              "      <td>0.180</td>\n",
              "      <td>0.048</td>\n",
              "      <td>5.114</td>\n",
              "      <td>101</td>\n",
              "      <td>1028</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.23</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.75</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>1.03</td>\n",
              "      <td>1.36</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.16</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.276</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.010</td>\n",
              "      <td>9.821</td>\n",
              "      <td>485</td>\n",
              "      <td>2259</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   word_freq_make  word_freq_address  ...  capital_run_length_total  spam\n",
              "0            0.00               0.64  ...                       278     1\n",
              "1            0.21               0.28  ...                      1028     1\n",
              "2            0.06               0.00  ...                      2259     1\n",
              "3            0.00               0.00  ...                       191     1\n",
              "\n",
              "[4 rows x 58 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the number of rows and columns:\n",
        "Spam.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2mu3qTLqaBB",
        "outputId": "e7d4a8df-30bd-4ee3-ba08-63eb44fe5b3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4601, 58)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Uniformity"
      ],
      "metadata": {
        "id": "_-z2YmqxfcPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Information about the data:\n",
        "Spam.info()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mH6tJeH4y_F6",
        "outputId": "abafd3c9-0e2f-4593-d40b-7f13beff880c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4601 entries, 0 to 4600\n",
            "Data columns (total 58 columns):\n",
            " #   Column                      Non-Null Count  Dtype  \n",
            "---  ------                      --------------  -----  \n",
            " 0   word_freq_make              4601 non-null   float64\n",
            " 1   word_freq_address           4601 non-null   float64\n",
            " 2   word_freq_all               4601 non-null   float64\n",
            " 3   word_freq_3d                4601 non-null   float64\n",
            " 4   word_freq_our               4601 non-null   float64\n",
            " 5   word_freq_over              4601 non-null   float64\n",
            " 6   word_freq_remove            4601 non-null   float64\n",
            " 7   word_freq_internet          4601 non-null   float64\n",
            " 8   word_freq_order             4601 non-null   float64\n",
            " 9   word_freq_mail              4601 non-null   float64\n",
            " 10  word_freq_receive           4601 non-null   float64\n",
            " 11  word_freq_will              4601 non-null   float64\n",
            " 12  word_freq_people            4601 non-null   float64\n",
            " 13  word_freq_report            4601 non-null   float64\n",
            " 14  word_freq_addresses         4601 non-null   float64\n",
            " 15  word_freq_free              4601 non-null   float64\n",
            " 16  word_freq_business          4601 non-null   float64\n",
            " 17  word_freq_email             4601 non-null   float64\n",
            " 18  word_freq_you               4601 non-null   float64\n",
            " 19  word_freq_credit            4601 non-null   float64\n",
            " 20  word_freq_your              4601 non-null   float64\n",
            " 21  word_freq_font              4601 non-null   float64\n",
            " 22  word_freq_000               4601 non-null   float64\n",
            " 23  word_freq_money             4601 non-null   float64\n",
            " 24  word_freq_hp                4601 non-null   float64\n",
            " 25  word_freq_hpl               4601 non-null   float64\n",
            " 26  word_freq_george            4601 non-null   float64\n",
            " 27  word_freq_650               4601 non-null   float64\n",
            " 28  word_freq_lab               4601 non-null   float64\n",
            " 29  word_freq_labs              4601 non-null   float64\n",
            " 30  word_freq_telnet            4601 non-null   float64\n",
            " 31  word_freq_857               4601 non-null   float64\n",
            " 32  word_freq_data              4601 non-null   float64\n",
            " 33  word_freq_415               4601 non-null   float64\n",
            " 34  word_freq_85                4601 non-null   float64\n",
            " 35  word_freq_technology        4601 non-null   float64\n",
            " 36  word_freq_1999              4601 non-null   float64\n",
            " 37  word_freq_parts             4601 non-null   float64\n",
            " 38  word_freq_pm                4601 non-null   float64\n",
            " 39  word_freq_direct            4601 non-null   float64\n",
            " 40  word_freq_cs                4601 non-null   float64\n",
            " 41  word_freq_meeting           4601 non-null   float64\n",
            " 42  word_freq_original          4601 non-null   float64\n",
            " 43  word_freq_project           4601 non-null   float64\n",
            " 44  word_freq_re                4601 non-null   float64\n",
            " 45  word_freq_edu               4601 non-null   float64\n",
            " 46  word_freq_table             4601 non-null   float64\n",
            " 47  word_freq_conference        4601 non-null   float64\n",
            " 48  char_freq_;                 4601 non-null   float64\n",
            " 49  char_freq_(                 4601 non-null   float64\n",
            " 50  char_freq_[                 4601 non-null   float64\n",
            " 51  char_freq_!                 4601 non-null   float64\n",
            " 52  char_freq_$                 4601 non-null   float64\n",
            " 53  char_freq_#                 4601 non-null   float64\n",
            " 54  capital_run_length_average  4601 non-null   float64\n",
            " 55  capital_run_length_longest  4601 non-null   int64  \n",
            " 56  capital_run_length_total    4601 non-null   int64  \n",
            " 57  spam                        4601 non-null   int64  \n",
            "dtypes: float64(55), int64(3)\n",
            "memory usage: 2.0 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Missing/Duplicate Values"
      ],
      "metadata": {
        "id": "If5be1kSzS96"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zuj4NtjA6jcv",
        "outputId": "3a73ec0b-5aa6-4935-d4f2-09f4b0f04f8c"
      },
      "source": [
        "#Checking for null entries in each column:\n",
        "Spam.isnull().sum()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "word_freq_make                0\n",
              "word_freq_address             0\n",
              "word_freq_all                 0\n",
              "word_freq_3d                  0\n",
              "word_freq_our                 0\n",
              "word_freq_over                0\n",
              "word_freq_remove              0\n",
              "word_freq_internet            0\n",
              "word_freq_order               0\n",
              "word_freq_mail                0\n",
              "word_freq_receive             0\n",
              "word_freq_will                0\n",
              "word_freq_people              0\n",
              "word_freq_report              0\n",
              "word_freq_addresses           0\n",
              "word_freq_free                0\n",
              "word_freq_business            0\n",
              "word_freq_email               0\n",
              "word_freq_you                 0\n",
              "word_freq_credit              0\n",
              "word_freq_your                0\n",
              "word_freq_font                0\n",
              "word_freq_000                 0\n",
              "word_freq_money               0\n",
              "word_freq_hp                  0\n",
              "word_freq_hpl                 0\n",
              "word_freq_george              0\n",
              "word_freq_650                 0\n",
              "word_freq_lab                 0\n",
              "word_freq_labs                0\n",
              "word_freq_telnet              0\n",
              "word_freq_857                 0\n",
              "word_freq_data                0\n",
              "word_freq_415                 0\n",
              "word_freq_85                  0\n",
              "word_freq_technology          0\n",
              "word_freq_1999                0\n",
              "word_freq_parts               0\n",
              "word_freq_pm                  0\n",
              "word_freq_direct              0\n",
              "word_freq_cs                  0\n",
              "word_freq_meeting             0\n",
              "word_freq_original            0\n",
              "word_freq_project             0\n",
              "word_freq_re                  0\n",
              "word_freq_edu                 0\n",
              "word_freq_table               0\n",
              "word_freq_conference          0\n",
              "char_freq_;                   0\n",
              "char_freq_(                   0\n",
              "char_freq_[                   0\n",
              "char_freq_!                   0\n",
              "char_freq_$                   0\n",
              "char_freq_#                   0\n",
              "capital_run_length_average    0\n",
              "capital_run_length_longest    0\n",
              "capital_run_length_total      0\n",
              "spam                          0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking for duplicates:\n",
        "Spam.duplicated().sum()     #391 duplicates identified\n",
        "\n",
        "#Dropping duplicates:\n",
        "Spam.drop_duplicates(inplace=True)\n",
        "Spam.duplicated().sum()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tc5yeKzpz8Bn",
        "outputId": "107884d2-fc35-4041-9cf4-4e8d1d1f93c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUx4vSj-QjEB"
      },
      "source": [
        "# **Data Exploration**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDQxWCJxR9vV"
      },
      "source": [
        "1. **Univariate Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk2CtohOR5Ik"
      },
      "source": [
        "This analysis describes the data in terms of:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-7_3m6mbEIo"
      },
      "source": [
        " - Spam/Non-Spam distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "FTlK0GgNN0lG",
        "outputId": "cae0dc7b-c73b-4725-9d0c-e81796b5ae7c"
      },
      "source": [
        "#Viewing the data in terms of spams and non-spams:\n",
        "spam = Spam['spam'].value_counts()\n",
        "spam\n",
        "# 0: 2,531(Non-spam)      1:  1,679(Spam)\n",
        "\n",
        "#Visualising the information on a pie chart:\n",
        "labels= ['Non-Spam','Spam']\n",
        "colors=['papayawhip', 'peachpuff']\n",
        "fontsize = 16\n",
        "plt.figure(figsize= (9, 5))\n",
        "plt.title(\"Spam Vs Non-Spam\", fontsize = fontsize)\n",
        "plt.pie(spam, labels=labels, colors=colors, startangle=180, shadow=True,explode=(0, 0.1), autopct='%1.2f%%')\n",
        "plt.axis('equal')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 648x360 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "40% of the E-mails are classified as Spam"
      ],
      "metadata": {
        "id": "wubLTBjdoo3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Descriptive Summary"
      ],
      "metadata": {
        "id": "BvRcc5XwrBAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#To view the descriptive summary:\n",
        "Spam.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "MsldBSM9rF5O",
        "outputId": "1f48e656-8520-4a50-adc8-1ab37dd6d412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_freq_make</th>\n",
              "      <th>word_freq_address</th>\n",
              "      <th>word_freq_all</th>\n",
              "      <th>word_freq_3d</th>\n",
              "      <th>word_freq_our</th>\n",
              "      <th>word_freq_over</th>\n",
              "      <th>word_freq_remove</th>\n",
              "      <th>word_freq_internet</th>\n",
              "      <th>word_freq_order</th>\n",
              "      <th>word_freq_mail</th>\n",
              "      <th>word_freq_receive</th>\n",
              "      <th>word_freq_will</th>\n",
              "      <th>word_freq_people</th>\n",
              "      <th>word_freq_report</th>\n",
              "      <th>word_freq_addresses</th>\n",
              "      <th>word_freq_free</th>\n",
              "      <th>word_freq_business</th>\n",
              "      <th>word_freq_email</th>\n",
              "      <th>word_freq_you</th>\n",
              "      <th>word_freq_credit</th>\n",
              "      <th>word_freq_your</th>\n",
              "      <th>word_freq_font</th>\n",
              "      <th>word_freq_000</th>\n",
              "      <th>word_freq_money</th>\n",
              "      <th>word_freq_hp</th>\n",
              "      <th>word_freq_hpl</th>\n",
              "      <th>word_freq_george</th>\n",
              "      <th>word_freq_650</th>\n",
              "      <th>word_freq_lab</th>\n",
              "      <th>word_freq_labs</th>\n",
              "      <th>word_freq_telnet</th>\n",
              "      <th>word_freq_857</th>\n",
              "      <th>word_freq_data</th>\n",
              "      <th>word_freq_415</th>\n",
              "      <th>word_freq_85</th>\n",
              "      <th>word_freq_technology</th>\n",
              "      <th>word_freq_1999</th>\n",
              "      <th>word_freq_parts</th>\n",
              "      <th>word_freq_pm</th>\n",
              "      <th>word_freq_direct</th>\n",
              "      <th>word_freq_cs</th>\n",
              "      <th>word_freq_meeting</th>\n",
              "      <th>word_freq_original</th>\n",
              "      <th>word_freq_project</th>\n",
              "      <th>word_freq_re</th>\n",
              "      <th>word_freq_edu</th>\n",
              "      <th>word_freq_table</th>\n",
              "      <th>word_freq_conference</th>\n",
              "      <th>char_freq_;</th>\n",
              "      <th>char_freq_(</th>\n",
              "      <th>char_freq_[</th>\n",
              "      <th>char_freq_!</th>\n",
              "      <th>char_freq_$</th>\n",
              "      <th>char_freq_#</th>\n",
              "      <th>capital_run_length_average</th>\n",
              "      <th>capital_run_length_longest</th>\n",
              "      <th>capital_run_length_total</th>\n",
              "      <th>spam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "      <td>4210.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.104366</td>\n",
              "      <td>0.112656</td>\n",
              "      <td>0.291473</td>\n",
              "      <td>0.063078</td>\n",
              "      <td>0.325321</td>\n",
              "      <td>0.096656</td>\n",
              "      <td>0.117475</td>\n",
              "      <td>0.108000</td>\n",
              "      <td>0.091860</td>\n",
              "      <td>0.248420</td>\n",
              "      <td>0.056686</td>\n",
              "      <td>0.565204</td>\n",
              "      <td>0.097656</td>\n",
              "      <td>0.061211</td>\n",
              "      <td>0.044803</td>\n",
              "      <td>0.253829</td>\n",
              "      <td>0.149154</td>\n",
              "      <td>0.188755</td>\n",
              "      <td>1.718368</td>\n",
              "      <td>0.084962</td>\n",
              "      <td>0.810040</td>\n",
              "      <td>0.131587</td>\n",
              "      <td>0.100622</td>\n",
              "      <td>0.090969</td>\n",
              "      <td>0.573995</td>\n",
              "      <td>0.279330</td>\n",
              "      <td>0.369838</td>\n",
              "      <td>0.128575</td>\n",
              "      <td>0.098382</td>\n",
              "      <td>0.103779</td>\n",
              "      <td>0.063544</td>\n",
              "      <td>0.044917</td>\n",
              "      <td>0.102333</td>\n",
              "      <td>0.045777</td>\n",
              "      <td>0.107881</td>\n",
              "      <td>0.099368</td>\n",
              "      <td>0.143482</td>\n",
              "      <td>0.014190</td>\n",
              "      <td>0.084057</td>\n",
              "      <td>0.061278</td>\n",
              "      <td>0.041432</td>\n",
              "      <td>0.141542</td>\n",
              "      <td>0.048772</td>\n",
              "      <td>0.085266</td>\n",
              "      <td>0.320124</td>\n",
              "      <td>0.189375</td>\n",
              "      <td>0.005786</td>\n",
              "      <td>0.034746</td>\n",
              "      <td>0.040403</td>\n",
              "      <td>0.144048</td>\n",
              "      <td>0.017376</td>\n",
              "      <td>0.281136</td>\n",
              "      <td>0.076057</td>\n",
              "      <td>0.045798</td>\n",
              "      <td>5.383896</td>\n",
              "      <td>52.139905</td>\n",
              "      <td>291.181948</td>\n",
              "      <td>0.398812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.300005</td>\n",
              "      <td>0.454260</td>\n",
              "      <td>0.515719</td>\n",
              "      <td>1.352487</td>\n",
              "      <td>0.687805</td>\n",
              "      <td>0.276030</td>\n",
              "      <td>0.397284</td>\n",
              "      <td>0.410282</td>\n",
              "      <td>0.282144</td>\n",
              "      <td>0.656638</td>\n",
              "      <td>0.184167</td>\n",
              "      <td>0.882513</td>\n",
              "      <td>0.309309</td>\n",
              "      <td>0.346066</td>\n",
              "      <td>0.242186</td>\n",
              "      <td>0.797534</td>\n",
              "      <td>0.457669</td>\n",
              "      <td>0.541133</td>\n",
              "      <td>1.768760</td>\n",
              "      <td>0.505583</td>\n",
              "      <td>1.149352</td>\n",
              "      <td>1.071258</td>\n",
              "      <td>0.351336</td>\n",
              "      <td>0.431723</td>\n",
              "      <td>1.676539</td>\n",
              "      <td>0.905553</td>\n",
              "      <td>1.775330</td>\n",
              "      <td>0.535429</td>\n",
              "      <td>0.576796</td>\n",
              "      <td>0.439568</td>\n",
              "      <td>0.387267</td>\n",
              "      <td>0.300748</td>\n",
              "      <td>0.569146</td>\n",
              "      <td>0.301812</td>\n",
              "      <td>0.530263</td>\n",
              "      <td>0.386470</td>\n",
              "      <td>0.424508</td>\n",
              "      <td>0.230512</td>\n",
              "      <td>0.447991</td>\n",
              "      <td>0.319520</td>\n",
              "      <td>0.318177</td>\n",
              "      <td>0.793803</td>\n",
              "      <td>0.231871</td>\n",
              "      <td>0.646096</td>\n",
              "      <td>1.045914</td>\n",
              "      <td>0.928308</td>\n",
              "      <td>0.079170</td>\n",
              "      <td>0.298521</td>\n",
              "      <td>0.252533</td>\n",
              "      <td>0.274256</td>\n",
              "      <td>0.105731</td>\n",
              "      <td>0.843321</td>\n",
              "      <td>0.239708</td>\n",
              "      <td>0.435925</td>\n",
              "      <td>33.147358</td>\n",
              "      <td>199.582168</td>\n",
              "      <td>618.654838</td>\n",
              "      <td>0.489712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.627500</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>40.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.190000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.360000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.290000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.073000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.016000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.297000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>101.500000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.440000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.410000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.190000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.830000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.127500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.720000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.280000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.097500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.170000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.194000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.331000</td>\n",
              "      <td>0.053000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.706750</td>\n",
              "      <td>44.000000</td>\n",
              "      <td>273.750000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>4.540000</td>\n",
              "      <td>14.280000</td>\n",
              "      <td>5.100000</td>\n",
              "      <td>42.810000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>5.880000</td>\n",
              "      <td>7.270000</td>\n",
              "      <td>11.110000</td>\n",
              "      <td>5.260000</td>\n",
              "      <td>18.180000</td>\n",
              "      <td>2.610000</td>\n",
              "      <td>9.670000</td>\n",
              "      <td>5.550000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>4.410000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>7.140000</td>\n",
              "      <td>9.090000</td>\n",
              "      <td>18.750000</td>\n",
              "      <td>18.180000</td>\n",
              "      <td>11.110000</td>\n",
              "      <td>17.100000</td>\n",
              "      <td>5.450000</td>\n",
              "      <td>12.500000</td>\n",
              "      <td>20.830000</td>\n",
              "      <td>16.660000</td>\n",
              "      <td>33.330000</td>\n",
              "      <td>9.090000</td>\n",
              "      <td>14.280000</td>\n",
              "      <td>5.880000</td>\n",
              "      <td>12.500000</td>\n",
              "      <td>4.760000</td>\n",
              "      <td>18.180000</td>\n",
              "      <td>4.760000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>7.690000</td>\n",
              "      <td>6.890000</td>\n",
              "      <td>8.330000</td>\n",
              "      <td>11.110000</td>\n",
              "      <td>4.760000</td>\n",
              "      <td>7.140000</td>\n",
              "      <td>14.280000</td>\n",
              "      <td>3.570000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>21.420000</td>\n",
              "      <td>22.050000</td>\n",
              "      <td>2.170000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>4.385000</td>\n",
              "      <td>9.752000</td>\n",
              "      <td>4.081000</td>\n",
              "      <td>32.478000</td>\n",
              "      <td>6.003000</td>\n",
              "      <td>19.829000</td>\n",
              "      <td>1102.500000</td>\n",
              "      <td>9989.000000</td>\n",
              "      <td>15841.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       word_freq_make  word_freq_address  ...  capital_run_length_total         spam\n",
              "count     4210.000000        4210.000000  ...               4210.000000  4210.000000\n",
              "mean         0.104366           0.112656  ...                291.181948     0.398812\n",
              "std          0.300005           0.454260  ...                618.654838     0.489712\n",
              "min          0.000000           0.000000  ...                  1.000000     0.000000\n",
              "25%          0.000000           0.000000  ...                 40.000000     0.000000\n",
              "50%          0.000000           0.000000  ...                101.500000     0.000000\n",
              "75%          0.000000           0.000000  ...                273.750000     1.000000\n",
              "max          4.540000          14.280000  ...              15841.000000     1.000000\n",
              "\n",
              "[8 rows x 58 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "- From the descriptive summary, columns capturing information on the Capital run length longest and total have higher means of 52 and 291 respectiviely as compared with the Word and Character frequencies which range between 0.01 to 1.72. For this reason, Feature scaling during model training is necessary to avoid creating a bias.\n",
        "- 60% of the e-mails in the dataset are non-spam. With the class ratio of 60:40, no oversampling or undersampling technique is adopted since its considered a slight imbalance and cannot cause significant performance degradation. An ideal data sample ratio is 50:50 given 2 classes.\n"
      ],
      "metadata": {
        "id": "xf2qIl_ariLm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yhJkntzjB1Z"
      },
      "source": [
        "# **Modelling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "qyAQc0ZPm9pq",
        "outputId": "6e7c5cb2-6963-4ff3-89a5-af95c671ad8d"
      },
      "source": [
        "#1. Creating the modelling dataframe:\n",
        "Model = Spam.copy()\n",
        "Model.head(4)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_freq_make</th>\n",
              "      <th>word_freq_address</th>\n",
              "      <th>word_freq_all</th>\n",
              "      <th>word_freq_3d</th>\n",
              "      <th>word_freq_our</th>\n",
              "      <th>word_freq_over</th>\n",
              "      <th>word_freq_remove</th>\n",
              "      <th>word_freq_internet</th>\n",
              "      <th>word_freq_order</th>\n",
              "      <th>word_freq_mail</th>\n",
              "      <th>word_freq_receive</th>\n",
              "      <th>word_freq_will</th>\n",
              "      <th>word_freq_people</th>\n",
              "      <th>word_freq_report</th>\n",
              "      <th>word_freq_addresses</th>\n",
              "      <th>word_freq_free</th>\n",
              "      <th>word_freq_business</th>\n",
              "      <th>word_freq_email</th>\n",
              "      <th>word_freq_you</th>\n",
              "      <th>word_freq_credit</th>\n",
              "      <th>word_freq_your</th>\n",
              "      <th>word_freq_font</th>\n",
              "      <th>word_freq_000</th>\n",
              "      <th>word_freq_money</th>\n",
              "      <th>word_freq_hp</th>\n",
              "      <th>word_freq_hpl</th>\n",
              "      <th>word_freq_george</th>\n",
              "      <th>word_freq_650</th>\n",
              "      <th>word_freq_lab</th>\n",
              "      <th>word_freq_labs</th>\n",
              "      <th>word_freq_telnet</th>\n",
              "      <th>word_freq_857</th>\n",
              "      <th>word_freq_data</th>\n",
              "      <th>word_freq_415</th>\n",
              "      <th>word_freq_85</th>\n",
              "      <th>word_freq_technology</th>\n",
              "      <th>word_freq_1999</th>\n",
              "      <th>word_freq_parts</th>\n",
              "      <th>word_freq_pm</th>\n",
              "      <th>word_freq_direct</th>\n",
              "      <th>word_freq_cs</th>\n",
              "      <th>word_freq_meeting</th>\n",
              "      <th>word_freq_original</th>\n",
              "      <th>word_freq_project</th>\n",
              "      <th>word_freq_re</th>\n",
              "      <th>word_freq_edu</th>\n",
              "      <th>word_freq_table</th>\n",
              "      <th>word_freq_conference</th>\n",
              "      <th>char_freq_;</th>\n",
              "      <th>char_freq_(</th>\n",
              "      <th>char_freq_[</th>\n",
              "      <th>char_freq_!</th>\n",
              "      <th>char_freq_$</th>\n",
              "      <th>char_freq_#</th>\n",
              "      <th>capital_run_length_average</th>\n",
              "      <th>capital_run_length_longest</th>\n",
              "      <th>capital_run_length_total</th>\n",
              "      <th>spam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.29</td>\n",
              "      <td>1.93</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.778</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.756</td>\n",
              "      <td>61</td>\n",
              "      <td>278</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.21</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.28</td>\n",
              "      <td>3.47</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.59</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.372</td>\n",
              "      <td>0.180</td>\n",
              "      <td>0.048</td>\n",
              "      <td>5.114</td>\n",
              "      <td>101</td>\n",
              "      <td>1028</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.23</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.75</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>1.03</td>\n",
              "      <td>1.36</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.16</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.276</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.010</td>\n",
              "      <td>9.821</td>\n",
              "      <td>485</td>\n",
              "      <td>2259</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   word_freq_make  word_freq_address  ...  capital_run_length_total  spam\n",
              "0            0.00               0.64  ...                       278     1\n",
              "1            0.21               0.28  ...                      1028     1\n",
              "2            0.06               0.00  ...                      2259     1\n",
              "3            0.00               0.00  ...                       191     1\n",
              "\n",
              "[4 rows x 58 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Partitioning: 80-20**"
      ],
      "metadata": {
        "id": "EAE33IsIHakX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_l103lVCwqB"
      },
      "source": [
        "For comparison purposes, the Logistic Regression model shall be used as the baseline model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression**"
      ],
      "metadata": {
        "id": "0iWuBvorHuUe"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DscYwoB1Fvc",
        "outputId": "2ba1b1bf-bedf-40c8-b2d6-905c3aed3b5d"
      },
      "source": [
        "#Creating the Variables:\n",
        "X = Model.drop('spam', axis=1)\n",
        "y = Model['spam']\n",
        "\n",
        "#Splitting the data into train and test:\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state= 0)\n",
        "\n",
        "#Printing the Shapes of the split data:\n",
        "print(f'X_train : {X_train.shape}')\n",
        "print(f'y_train : {y_train.shape}')\n",
        "print(f'X_test : {X_test.shape}')\n",
        "print(f'y_test : {y_test.shape}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train : (3368, 57)\n",
            "y_train : (3368,)\n",
            "X_test : (842, 57)\n",
            "y_test : (842,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zHfMCoYHwJA"
      },
      "source": [
        "Building a logistic regression model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCnmiL5z11lg"
      },
      "source": [
        "#Instantiating an object Log:\n",
        "Log = LogisticRegression()\n",
        "\n",
        "#Fitting the model:\n",
        "Log.fit(X_train,y_train )\n",
        "\n",
        "#Applying the trained model to make a prediction:\n",
        "y_pred = Log.predict(X_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5dChB96YYHr"
      },
      "source": [
        "Performance Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZTaTr4l1-te",
        "outputId": "2a2b104b-d6d9-4ace-8c27-0ad1434deba7"
      },
      "source": [
        "#Checking the accuracy of the test and train models:\n",
        "print(f'Train Accuracy: {Log.score(X_train, y_train):.2f}')\n",
        "print(f'Test Accuracy: {Log.score(X_test, y_test):.2f}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.92\n",
            "Test Accuracy: 0.92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4vXPBFPbBq4"
      },
      "source": [
        "The Training and Test set accuracy are very much comparable.\n",
        "\n",
        "Since the model is doing better on both the training and test set, there's no case of Overfitting. The Score also shows that just about 8% of the emails have been misclassified"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NSBbef-xWKU",
        "outputId": "0d764f91-1d20-438d-c46c-0cb0eaff9e53"
      },
      "source": [
        "#Checking the classification report:\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "#Getting the Recall and Precision Scores:\n",
        "print('Precision Score: ', precision_score(y_test, y_pred).round(2))\n",
        "print('Recall Score: ', precision_score(y_test, y_pred).round(2))\n",
        "print('F1 Score: ', f1_score(y_test, y_pred).round(2))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.93      0.93       495\n",
            "           1       0.90      0.90      0.90       347\n",
            "\n",
            "    accuracy                           0.92       842\n",
            "   macro avg       0.91      0.91      0.91       842\n",
            "weighted avg       0.92      0.92      0.92       842\n",
            "\n",
            "Precision Score:  0.9\n",
            "Recall Score:  0.9\n",
            "F1 Score:  0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Using confusion matrix:\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOMvGTN850Pb",
        "outputId": "c30032e0-f72f-46b0-940a-2c39098b6fa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[460  35]\n",
            " [ 36 311]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "35 and 36 are misclassifications in the 2 classes"
      ],
      "metadata": {
        "id": "93JtgpqHtwvd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Naive Bayes Classifier**"
      ],
      "metadata": {
        "id": "-ChYsJfr-2am"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes’ theorem with strong independence assumptions between the features.\n",
        "\n",
        "The 3 different types of of Bayes classifiers are: Gaussian, Multinomial and Bernoulli.\n",
        "\n",
        "Gaussian is used on features that are continuous and their distribution is assumed to be Normal. Multinomial is mostly used for discrete counts or considers the frequency count/occurrences of the features {words in this case}. Bernoulli on the other hand cares about the presence or absence of a particular feature/word. It is adequate for features that are binary-valued.\n",
        "\n",
        "For this particluar data, since we are considering word/character frequencies, The Multinomial Bayes is adopted."
      ],
      "metadata": {
        "id": "fAv0rC6M9OZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Selection:\n",
        "Nom = MultinomialNB()\n",
        "\n",
        "#Fitting the model:\n",
        "Nom.fit(X_train,y_train )\n",
        "\n",
        "#Applying the trained model to make a prediction:\n",
        "y_pred = Nom.predict(X_test)"
      ],
      "metadata": {
        "id": "Sdvk0DK9_UVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the accuracy of the model:\n",
        "print(f'Train Accuracy: {Nom.score(X_train, y_train):.2f}')\n",
        "print(f'Test Accuracy: {Nom.score(X_test, y_test):.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3Pc2CQs_faH",
        "outputId": "74fc36e5-e1d3-4cef-8e41-5682594f07f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.79\n",
            "Test Accuracy: 0.77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The baseline model is performing better than the Naive bayes with an accuracy of 92%"
      ],
      "metadata": {
        "id": "kz3aRP3k8Tnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the classification report:\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "#Getting the Recall and Precision Scores:\n",
        "print('Precision Score: ', precision_score(y_test, y_pred).round(2))\n",
        "print('Recall Score: ', precision_score(y_test, y_pred).round(2))\n",
        "print('F1 Score: ', f1_score(y_test, y_pred).round(2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2lo2Egv8NO3",
        "outputId": "be884899-ceab-4733-9893-8d43c35e1cac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.84      0.81       495\n",
            "           1       0.74      0.67      0.71       347\n",
            "\n",
            "    accuracy                           0.77       842\n",
            "   macro avg       0.76      0.75      0.76       842\n",
            "weighted avg       0.77      0.77      0.77       842\n",
            "\n",
            "Precision Score:  0.74\n",
            "Recall Score:  0.74\n",
            "F1 Score:  0.71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Using confusion matrix:\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jy1cbs0g_pUP",
        "outputId": "41faa0c6-d93d-4680-fff5-10eedda66909"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[415  80]\n",
            " [114 233]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of misclassifications across the classes is quite high, hence the model is performing poorly when it comes to identifying an email as spam"
      ],
      "metadata": {
        "id": "3GJbiRcNvbHn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qBpl8ny3Uk2"
      },
      "source": [
        "**Hyperparameter Optimization**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see if any improvement is observed on the accuracy when Normalization is performed:"
      ],
      "metadata": {
        "id": "lXGoDR9E_Sqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performing Normalization"
      ],
      "metadata": {
        "id": "VHUwwhrnATtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature Scaling:\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.fit_transform(X_test)\n",
        "\n",
        "#Model Selection:\n",
        "Mn = MultinomialNB()\n",
        "\n",
        "#Fitting the model:\n",
        "Mn.fit(X_train,y_train )\n",
        "\n",
        "#Applying the trained model to make a prediction:\n",
        "Y_pred = Mn.predict(X_test)"
      ],
      "metadata": {
        "id": "Lky8ucLRAZVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaN4myYTu8Od",
        "outputId": "e4fa5050-5f65-4252-a3ea-0e97f62cf1f7"
      },
      "source": [
        "#Checking the Accuracy:\n",
        "print(f'Train Accuracy: {Mn.score(X_train, y_train):.2f}')\n",
        "Fore = print(f'Test Accuracy: {Mn.score(X_test, y_test):.2f}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.89\n",
            "Test Accuracy: 0.90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After optimization, the accuracy improves from a 77% to 90%"
      ],
      "metadata": {
        "id": "XGJlIcRsvu6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the classification report:\n",
        "print(classification_report(y_test, Y_pred))\n",
        "\n",
        "#Getting the Recall and Precision Scores:\n",
        "print('Precision Score: ', precision_score(y_test, Y_pred).round(2))\n",
        "print('Recall Score: ', precision_score(y_test, Y_pred).round(2))\n",
        "print('F1 Score: ', f1_score(y_test, Y_pred).round(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vM1NKW6f_4Ys",
        "outputId": "ba73941c-8efa-4b3a-fd7f-59144bd9b8bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.95      0.92       495\n",
            "           1       0.93      0.82      0.87       347\n",
            "\n",
            "    accuracy                           0.90       842\n",
            "   macro avg       0.90      0.89      0.89       842\n",
            "weighted avg       0.90      0.90      0.90       842\n",
            "\n",
            "Precision Score:  0.93\n",
            "Recall Score:  0.93\n",
            "F1 Score:  0.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MM_RDArk2uP",
        "outputId": "ea9b82e4-46ea-435b-d51c-0740b06ad9b2"
      },
      "source": [
        "#Using confusion matrix:\n",
        "print(confusion_matrix(y_test, Y_pred))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[472  23]\n",
            " [ 63 284]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "- The classifier made a total of 842 predictions.\n",
        "- Out of the 842 cases, the classifier predicted yes/Spammed 307 times and no/Non-spammed 535 times. In reality, there were 495 Non-spam emails  and 347 Spam emails.\n",
        "- There were 284 true negatives( cases where the actual and predicted values are no) and 472 true positives( cases where actual and predicted values are yes).\n",
        "- There were 23 false positives and 63 false negatives. These are the total misclassifications.\n",
        "- The true positive rate/recall score is 0.93. What this means is that, when the email is Spam, 90% of the times does the model predict correct.\n",
        "- When the model predicts Spammed, from the precision score, 93% of the times is correct.\n"
      ],
      "metadata": {
        "id": "M73BTH0Wt7ce"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPq77M_MGMAe"
      },
      "source": [
        "**Partitioning: 70 - 30**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Baseline Model**"
      ],
      "metadata": {
        "id": "d02S0Pa5yuKq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-To8h6DGLi4",
        "outputId": "74448e66-e26f-4353-dc84-d1ed03a9e9ff"
      },
      "source": [
        "#Creating the Variables:\n",
        "X = Model.drop('spam', axis=1)\n",
        "y = Model['spam']\n",
        "\n",
        "#Splitting the data into train and test:\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state= 0)\n",
        "\n",
        "#Printing the Shapes of the split data:\n",
        "print(f'X_train : {X_train.shape}')\n",
        "print(f'y_train : {y_train.shape}')\n",
        "print(f'X_test : {X_test.shape}')\n",
        "print(f'y_test : {y_test.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train : (2947, 57)\n",
            "y_train : (2947,)\n",
            "X_test : (1263, 57)\n",
            "y_test : (1263,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pK-z6dDIGW--"
      },
      "source": [
        "#Model Selection without any hyperparameters:\n",
        "Log2 = LogisticRegression()\n",
        "\n",
        "#Fitting the model:\n",
        "Log2.fit(X_train,y_train )\n",
        "\n",
        "#Applying the trained model to make a prediction:\n",
        "y_pred = Log2.predict(X_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM70RzZQx2Ty"
      },
      "source": [
        "Performance Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Yd5sZ0bGaUn",
        "outputId": "b30f0cd0-a30c-4405-f79b-c93bce3e7c92"
      },
      "source": [
        "#Checking the accuracy of the test and train models:\n",
        "print(f'Train Accuracy: {Log2.score(X_train, y_train):.2f}')\n",
        "print(f'Test Accuracy: {Log2.score(X_test, y_test):.2f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.92\n",
            "Test Accuracy: 0.92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Md6miMBbGdT2",
        "outputId": "89c0b1c1-e435-4837-c4f3-891047870b44"
      },
      "source": [
        "#Checking the classification report:\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "#Getting the Recall and Precision Scores:\n",
        "print('Precision Score: ', precision_score(y_test, y_pred).round(2))\n",
        "print('Recall Score: ', precision_score(y_test, y_pred).round(2))\n",
        "print('F1 Score: ', f1_score(y_test, y_pred).round(2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.92      0.93       737\n",
            "           1       0.89      0.91      0.90       526\n",
            "\n",
            "    accuracy                           0.92      1263\n",
            "   macro avg       0.91      0.91      0.91      1263\n",
            "weighted avg       0.92      0.92      0.92      1263\n",
            "\n",
            "Precision Score:  0.89\n",
            "Recall Score:  0.89\n",
            "F1 Score:  0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy score on a different test size remains same at 92%"
      ],
      "metadata": {
        "id": "S8we7kKjzJSg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Naive Bayes Classifier**"
      ],
      "metadata": {
        "id": "heElyeg6EK7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Selection:\n",
        "Nom2 = MultinomialNB()\n",
        "\n",
        "#Fitting the model:\n",
        "Nom2.fit(X_train,y_train )\n",
        "\n",
        "#Applying the trained model to make a prediction:\n",
        "y_pred = Nom2.predict(X_test)"
      ],
      "metadata": {
        "id": "BjugaKVxELtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the accuracy of the test and train models:\n",
        "print(f'Train Accuracy: {Nom2.score(X_train, y_train):.2f}')\n",
        "print(f'Test Accuracy: {Nom2.score(X_test, y_test):.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYo9Lf9SEZks",
        "outputId": "1bfe6b62-58af-4aeb-df51-bdee1db5ee96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.79\n",
            "Test Accuracy: 0.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the classification report:\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "#Getting the Recall and Precision Scores:\n",
        "print('Precision Score: ', precision_score(y_test, y_pred).round(2))\n",
        "print('Recall Score: ', precision_score(y_test, y_pred).round(2))\n",
        "print('F1 Score: ', f1_score(y_test, y_pred).round(2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97B6p9EPC1zQ",
        "outputId": "57c633ab-6cc4-4929-bfe9-8156f88a324d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.85      0.82       737\n",
            "           1       0.76      0.69      0.73       526\n",
            "\n",
            "    accuracy                           0.78      1263\n",
            "   macro avg       0.78      0.77      0.77      1263\n",
            "weighted avg       0.78      0.78      0.78      1263\n",
            "\n",
            "Precision Score:  0.76\n",
            "Recall Score:  0.76\n",
            "F1 Score:  0.73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Using confusion matrix:\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVY1sdrcEWpx",
        "outputId": "c5f1978e-6997-4823-e530-cffd96b3ed11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[624 113]\n",
            " [162 364]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The baseline model is still performing better than the Bayes model before optimization with an accuracy of 92%"
      ],
      "metadata": {
        "id": "-eq9ljLNz4bl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvUPTn6gx9Y1"
      },
      "source": [
        "**Hyperparameter Optimization**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature Scaling:\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.fit_transform(X_test)\n",
        "\n",
        "#Model Selection:\n",
        "Mn2 = MultinomialNB()\n",
        "\n",
        "#Fitting the model:\n",
        "Mn2.fit(X_train,y_train )\n",
        "\n",
        "#Applying the trained model to make a prediction:\n",
        "Y_pred = Mn2.predict(X_test)"
      ],
      "metadata": {
        "id": "0xkQ8nWmE26s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the accuracy of the test and train models:\n",
        "print(f'Train Accuracy: {Mn2.score(X_train, y_train):.2f}')\n",
        "print(f'Test Accuracy: {Mn2.score(X_test, y_test):.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXd0c-0VE7OZ",
        "outputId": "2d2e947a-9a88-4d09-e6a2-201197d86720"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.88\n",
            "Test Accuracy: 0.90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the classification report:\n",
        "print(classification_report(y_test, Y_pred))\n",
        "\n",
        "#Getting the Recall and Precision Scores:\n",
        "print('Precision Score: ', precision_score(y_test, Y_pred).round(2))\n",
        "print('Recall Score: ', precision_score(y_test, Y_pred).round(2))\n",
        "print('F1 Score: ', f1_score(y_test, Y_pred).round(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38NdHmneDC2S",
        "outputId": "6c9d5107-4f2f-4632-cd24-49dc31106881"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.96      0.92       737\n",
            "           1       0.93      0.82      0.87       526\n",
            "\n",
            "    accuracy                           0.90      1263\n",
            "   macro avg       0.91      0.89      0.90      1263\n",
            "weighted avg       0.90      0.90      0.90      1263\n",
            "\n",
            "Precision Score:  0.93\n",
            "Recall Score:  0.93\n",
            "F1 Score:  0.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Using confusion matrix:\n",
        "print(confusion_matrix(y_test, Y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrcICFXgFARw",
        "outputId": "797b1767-43de-42e5-b26c-71c79f6ade66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[705  32]\n",
            " [ 94 432]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "- The classifier made a total of 1,263 predictions.\n",
        "- Out of the 1,263 cases, the classifier predicted Spammed 464 times and Non-spammed 799 times. In reality, there were 737 Non-spammed emails  and 526 Spammed emails. The predictor is likely to always predict a given email as Non-spam. \n",
        "- There were 432 true negatives and 705 true positives.\n",
        "- There were 32 false positives and 94 false negatives. These are the total misclassifications.\n",
        "- The true positive rate is 0.93.\n"
      ],
      "metadata": {
        "id": "xeS17Er30b0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Partitioning: 60 - 40**"
      ],
      "metadata": {
        "id": "K1OiH0A8FZL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Baseline Model**"
      ],
      "metadata": {
        "id": "1dRG7Z-F2g05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating the Variables:\n",
        "X = Model.drop('spam', axis=1)\n",
        "y = Model['spam']\n",
        "\n",
        "#Splitting the data into train and test:\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state= 0)\n",
        "\n",
        "#Printing the Shapes of the split data:\n",
        "print(f'X_train : {X_train.shape}')\n",
        "print(f'y_train : {y_train.shape}')\n",
        "print(f'X_test : {X_test.shape}')\n",
        "print(f'y_test : {y_test.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PREcIrdFfFn",
        "outputId": "ba017266-46c3-4c36-cc71-35353d55c3f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train : (2526, 57)\n",
            "y_train : (2526,)\n",
            "X_test : (1684, 57)\n",
            "y_test : (1684,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Selection:\n",
        "Log3 = LogisticRegression()\n",
        "\n",
        "#Fitting the model:\n",
        "Log3.fit(X_train,y_train )\n",
        "\n",
        "#Applying the trained model to make a prediction:\n",
        "y_pred = Log3.predict(X_test)\n"
      ],
      "metadata": {
        "id": "OrKgwhmUFlQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aheAU6i_3UaE"
      },
      "source": [
        "Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmPMyq_SG8ZY",
        "outputId": "dc16b831-d44a-4752-c26c-3afb5c3038bc"
      },
      "source": [
        "#Checking the accuracy of the test and train models:\n",
        "print(f'Train Accuracy: {Log3.score(X_train, y_train):.2f}')\n",
        "print(f'Test Accuracy: {Log3.score(X_test, y_test):.2f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.93\n",
            "Test Accuracy: 0.92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the classification report:\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "#Getting the Recall and Precision Scores:\n",
        "print('Precision Score: ', precision_score(y_test, y_pred).round(2))\n",
        "print('Recall Score: ', precision_score(y_test, y_pred).round(2))\n",
        "print('F1 Score: ', f1_score(y_test, y_pred).round(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Tkq_qs3DOoi",
        "outputId": "683f7c72-49fc-4e39-a3db-d208594a97db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.93      0.93       994\n",
            "           1       0.89      0.90      0.90       690\n",
            "\n",
            "    accuracy                           0.92      1684\n",
            "   macro avg       0.91      0.91      0.91      1684\n",
            "weighted avg       0.92      0.92      0.92      1684\n",
            "\n",
            "Precision Score:  0.89\n",
            "Recall Score:  0.89\n",
            "F1 Score:  0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvPY0oY33XNK",
        "outputId": "1205d035-9b1f-4682-eda7-882ed6702c1b"
      },
      "source": [
        "#Using confusion matrix:\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[920  74]\n",
            " [ 67 623]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Naive Bayes Classifier**"
      ],
      "metadata": {
        "id": "9nI5nVRaGKNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Selection:\n",
        "Nom3 = MultinomialNB()\n",
        "\n",
        "#Fitting the model:\n",
        "Nom3.fit(X_train,y_train )\n",
        "\n",
        "#Applying the trained model to make a prediction:\n",
        "y_pred = Nom3.predict(X_test)"
      ],
      "metadata": {
        "id": "rGtRx5UBGJqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the accuracy of the test and train models:\n",
        "print(f'Train Accuracy: {Nom3.score(X_train, y_train):.2f}')\n",
        "print(f'Test Accuracy: {Nom3.score(X_test, y_test):.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aE5eN7opGYgV",
        "outputId": "172dcf13-d74d-41be-d4be-02d85ddf64c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.79\n",
            "Test Accuracy: 0.79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the classification report:\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "#Getting the Recall and Precision Scores:\n",
        "print('Precision Score: ', precision_score(y_test, y_pred).round(2))\n",
        "print('Recall Score: ', precision_score(y_test, y_pred).round(2))\n",
        "print('F1 Score: ', f1_score(y_test, y_pred).round(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lApTLzCdDQXN",
        "outputId": "c4304cf8-084f-4ecb-d1c0-20d09f7e423a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.84      0.83       994\n",
            "           1       0.76      0.72      0.74       690\n",
            "\n",
            "    accuracy                           0.79      1684\n",
            "   macro avg       0.79      0.78      0.78      1684\n",
            "weighted avg       0.79      0.79      0.79      1684\n",
            "\n",
            "Precision Score:  0.76\n",
            "Recall Score:  0.76\n",
            "F1 Score:  0.74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Using confusion matrix:\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUIARiRIGcz5",
        "outputId": "fa03b294-f658-4dc7-9987-7c33b84945f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[838 156]\n",
            " [192 498]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameter Optimization**"
      ],
      "metadata": {
        "id": "wzqSl0zWGhfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature Scaling:\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.fit_transform(X_test)\n",
        "\n",
        "#Model Selection:\n",
        "Mn3 = MultinomialNB()\n",
        "\n",
        "#Fitting the model:\n",
        "Mn3.fit(X_train,y_train )\n",
        "\n",
        "#Applying the trained model to make a prediction:\n",
        "Y_pred = Mn3.predict(X_test)"
      ],
      "metadata": {
        "id": "57wqgXLvGS5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the accuracy of the test and train models:\n",
        "print(f'Train Accuracy: {Mn3.score(X_train, y_train):.2f}')\n",
        "print(f'Test Accuracy: {Mn3.score(X_test, y_test):.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrUcwVWRGkKV",
        "outputId": "5fcf1d72-4df1-4d22-bae6-01d0e3e4ca9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.89\n",
            "Test Accuracy: 0.90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the classification report:\n",
        "print(classification_report(y_test, Y_pred))\n",
        "\n",
        "#Getting the Recall and Precision Scores:\n",
        "print('Precision Score: ', precision_score(y_test, y_pred).round(2))\n",
        "print('Recall Score: ', precision_score(y_test, y_pred).round(2))\n",
        "print('F1 Score: ', f1_score(y_test, y_pred).round(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NGMH4tQDSXS",
        "outputId": "bb79c1a8-1923-4855-a6e1-505859738965"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.96      0.92       994\n",
            "           1       0.93      0.81      0.87       690\n",
            "\n",
            "    accuracy                           0.90      1684\n",
            "   macro avg       0.91      0.89      0.89      1684\n",
            "weighted avg       0.90      0.90      0.90      1684\n",
            "\n",
            "Precision Score:  0.76\n",
            "Recall Score:  0.76\n",
            "F1 Score:  0.74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Using confusion matrix:\n",
        "print(confusion_matrix(y_test, Y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNhy512DGnOR",
        "outputId": "c54bf1b5-661a-4f03-d565-cd1b54718972"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[953  41]\n",
            " [128 562]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "- The classifier made a total of 1,684 predictions.\n",
        "- Out of the 1,684 cases, the classifier predicted Spammed 603 times and Non-spammed 1,081 times. In reality, there were 994 Non-spammed emails  and 690 Spammed emails. \n",
        "- There were 562 true negatives and 953 true positives.These are the correct classifications.\n",
        "- There were 41 false positives and 128 false negatives. These are the total misclassifications.\n",
        "- The true positive rate is 0.76.\n"
      ],
      "metadata": {
        "id": "AHJfi1tT4aUn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Performance Analysis**"
      ],
      "metadata": {
        "id": "MIFe8-f75xXZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assessing Accuracy**"
      ],
      "metadata": {
        "id": "EWIO5BUT52kb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- From the Multinomial Naive Bayes model, the accuracy score across the different test sizes remained the same.\n",
        "- The Naive model performed better after feature scaling. Before normalization, the accuracy ranged between 77-79%, after, the performance improved to 90%\n",
        "- In the first 2 partitioning: 80-20 and 70-30, the true positve rate was at 0.93. In the last one of 60-40, the rate was 0.76. As the training data reduced, so did the scores. Generalizing the change, with more training data, the model is able to learn the underlying distribution of the real data better. The first 2 partitionings in this case are preferred due to better scores"
      ],
      "metadata": {
        "id": "NLH1CThd56nh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusions and Recommendations**"
      ],
      "metadata": {
        "id": "jklxJUjI96hQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Naive Bayes delivered same results despite the size variation. Adding training data does not make the model any better. This makes it an easy algorithm to train as it can thrive with training on a very small and limited dataset.\n",
        "- There is no risk of overfitting in this model since its not sensitive to noisy features that are irrelevant.\n",
        "- To Improve the classifier:\n",
        " - Feature scaling is highly recommended.\n",
        " - Since the classifier assumes each feature makes an independent and equal contribution to the outcome, its good to test for any dependency. Checking for any irrelevant features during data cleaning can also be tested to see if the scores improve further.\n"
      ],
      "metadata": {
        "id": "8lk4whVE-D1Y"
      }
    }
  ]
}
